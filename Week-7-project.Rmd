# Week 7 Project

## Tristan Truttmann

<!-- leave this material as is -->
Compiled on `r date()`.

```{r include = FALSE}
library(DataComputing)
```
<!-- put your content after this line -->

```{r}
Wake_county <- read.csv("http://tiny.cc/dcf/street-addresses.csv",stringsAsFactors = FALSE)
download.file(url="http://tiny.cc/dcf/CMS_ProvidersSimple.rds",destfile="YourNameForTheFile.rds")
Medicare <- readRDS("YourNameForTheFile.rds") 
```


```{r}
 # This defines a regular expressio nthat is used to remove labels we have already eliminated
pattern <- "(ST|RD|ROAD|CIRCLE|WAY|TRAIL|AVE|LN|DR|CT|COURT|CIR|LANE|PKWY|PL|PATH|RUN|TERRACE|PARKWA|CV|CO|HWY|WYND|PT|BRISBAYNE|UNIVERSITY|SUITE|CUTHBERT|MOUNTAIN|TERA|UNION|HARBORVIEW|MAR|MANOR|TER|WHARF|DORM|PASS|MEADOW|HEIGHTS|CREEK|PARK|L)"
junk <- "BOX|WINERY|HALL|BAREFOOT|LIVE OAK|6516|BX|OAKS|PO|HMIA-267|HMIA-267|B0X|CHARONNE"
LeftOvers <- # This createds a table for the labels that we haven't eliminated yet. 
  Wake_county %>% # We take the data we downloaded... 
  filter( ! grepl(pattern,address) , # ... then we remove ones that have the pattern defined above
          ! grepl(junk, address) # ... then we remove PO Boxes
        )
nrow(LeftOvers)
head(LeftOvers,16)
```
I just kept iterating until I had 71 rows left, and then I read through the rows manually to see which ones were not junk.  When I say junk I mean addresses that don't contain any of these labels.  

Then to count how many there are of each, first I assign the varaible `street_word` to the labels, then I use group_by and summarize to count how many there are of each: 

```{r}
WithLabels <- # This defines a new table with the `street_word` variable
  Wake_county %>% # It starts with the table from Wake_county
  tidyr::extract(address, into = "street_word",regex = pattern, remove=FALSE) # And then it extracts the regex from the address variable and assigns them to the street_word variable. 
head(WithLabels)
WithLabels %>% 
  group_by(street_word) %>%
  summarize(number = n()) %>%
  arrange(desc(number))
```

As you can see from the results, it appears that `L` is the most common with 1585, `ST` is a the second most common with 662, and `RD` is the thirs most with 377.  

